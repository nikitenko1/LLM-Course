{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Weâ€™ll cover the following models and their corresponding tasks:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Wav2Vec2 for audio classification and automatic speech recognition (ASR)\n",
    "* Vision Transformer (ViT) and ConvNeXT for image classification\n",
    "* DETR for object detection\n",
    "* Mask2Former for image segmentation\n",
    "* GLPN for depth estimation\n",
    "* BERT for NLP tasks like text classification, token classification and question answering that use an encoder\n",
    "* GPT2 for NLP tasks like text generation that use a decoder\n",
    "* BART for NLP tasks like summarization and translation that use an encoder-decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feecbed",
   "metadata": {},
   "source": [
    "#### @Types of language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Encoder-only models (like BERT): These models use a bidirectional approach to understand context from both directions. \n",
    "Theyâ€™re best suited for tasks that require deep understanding of text, such as classification, named entity recognition, \n",
    "and question answering.\n",
    "\n",
    "* Decoder-only models (like GPT, Llama): These models process text from left to right and are particularly good \n",
    "at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt.\n",
    "\n",
    "* Encoder-decoder models (like T5, BART): These models combine both approaches, using an encoder to understand \n",
    "the input and a decoder to generate output. \n",
    "They excel at sequence-to-sequence tasks like translation, summarization, and question answering.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b399ef",
   "metadata": {},
   "source": [
    "![Sample Image](images/transformers_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751902e3",
   "metadata": {},
   "source": [
    "> Understanding which part of the Transformer architecture (encoder, decoder, or both) is best suited for a particular NLP task is key to choosing the right model. Generally, tasks requiring bidirectional context use encoders, tasks generating text use decoders, and tasks converting one sequence to another use encoder-decoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb92cb",
   "metadata": {},
   "source": [
    "#### @Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* GPT-2 is a decoder-only model pretrained on a large amount of text. \n",
    "It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks \n",
    "like question answering despite not being explicitly trained to.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da694c41",
   "metadata": {},
   "source": [
    "#### @Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Text classification involves assigning predefined categories to text documents, \n",
    "such as sentiment analysis, topic classification, or spam detection.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* BERT is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn \n",
    "richer representations of the text by attending to words on both sides.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb1d14",
   "metadata": {},
   "source": [
    "#### @Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705cb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Token classification involves assigning a label to each token in a sequence, \n",
    "such as in named entity recognition or part-of-speech tagging.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To use BERT for token classification tasks like named entity recognition (NER), \n",
    "add a token classification head on top of the base BERT model. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b5a13",
   "metadata": {},
   "source": [
    "#### @Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question answering involves finding the answer to a question within a given context or passage.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To use BERT for question answering, add a span classification head on top of the base BERT model.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb154a5",
   "metadata": {},
   "source": [
    "ðŸ’¡ Notice how easy it is to use BERT for different tasks once itâ€™s been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f116ae8",
   "metadata": {},
   "source": [
    "#### @Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7825d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summarization involves condensing a longer text into a shorter version while preserving its key information and meaning.\n",
    "\n",
    "Encoder-decoder models like BART and T5 are designed for the sequence-to-sequence pattern of a summarization task.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349bc165",
   "metadata": {},
   "source": [
    "#### @Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e83a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Translation involves converting text from one language to another while preserving its meaning. Translation is another example of a sequence-to-sequence task, \n",
    "which means you can use an encoder-decoder model like BART or T5 to do it\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc74b16",
   "metadata": {},
   "source": [
    "#### @Speech and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdee95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Whisper is a encoder-decoder (sequence-to-sequence) transformer pretrained on 680,000 hours of labeled audio data. \n",
    "This amount of pretraining data enables zero-shot performance on audio tasks in English and many other languages\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f171b2",
   "metadata": {},
   "source": [
    "![Sample Image](images/whisper_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This model has two main components:\n",
    "\n",
    "* An encoder processes the input audio. The raw audio is first converted into a log-Mel spectrogram. \n",
    "This spectrogram is then passed through a Transformer encoder network.\n",
    "\n",
    "* A decoder takes the encoded audio representation and autoregressively predicts the corresponding text tokens.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec829e2f",
   "metadata": {},
   "source": [
    "#### @Automatic speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbdca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To use the pretrained model for automatic speech recognition, you leverage its full encoder-decoder structure. \n",
    "The encoder processes the audio input, and the decoder autoregressively generates the transcript token by token.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622953aa",
   "metadata": {},
   "source": [
    "#### @Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There are two ways to approach computer vision tasks:\n",
    "\n",
    "* Split an image into a sequence of patches and process them in parallel with a Transformer.\n",
    "* Use a modern CNN, like ConvNeXT, which relies on convolutional layers but adopts modern network designs.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c76da",
   "metadata": {},
   "source": [
    "> A third approach mixes Transformers with convolutions (for example, Convolutional Vision Transformer or LeViT). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231444ca",
   "metadata": {},
   "source": [
    "#### @Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bff572",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism\n",
    "while ConvNeXT uses convolutions.\n",
    "\n",
    "ViT replaces convolutions entirely with a pure Transformer architecture. If youâ€™re familiar with the original Transformer, then youâ€™re already most of the way toward understanding ViT.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55dd5d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
